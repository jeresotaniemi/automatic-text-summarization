{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f83328",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sumy rouge-score nltk glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ae305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and global variables\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "# Initialize language for stemmer and the number of most important sentences returned\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 3\n",
    "\n",
    "# **IMPORTANT Note**: Since we are using Sumy package instead of the original PyTLDR, there is no direct counterpart for Relevance sentence scoring. We use the closest equivalent LexRank which also uses cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e7a428",
   "metadata": {},
   "source": [
    "### Task 1: Three summarization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3348da79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top 3 Most Important Sentences by Each Algorithm ---\n",
      "\n",
      "TextRank Summary:\n",
      "Natural language processing and text mining is a course in Master's degree program in the Univeristy of Oulu.\n",
      "Video games are good for passing time efficiently.\n",
      "University of Oulu has multiple restaurants which are cheap and offer healthy food.\n",
      "\n",
      "LSA Summary:\n",
      "Natural language processing and text mining is a course in Master's degree program in the Univeristy of Oulu.\n",
      "University of Oulu has multiple restaurants which are cheap and offer healthy food.\n",
      "Multiple different courses require experience on programming languages.\n",
      "\n",
      "LexRank Summary:\n",
      "Natural language processing and text mining is a course in Master's degree program in the Univeristy of Oulu.\n",
      "Video games are good for passing time efficiently.\n",
      "University of Oulu has multiple restaurants which are cheap and offer healthy food.\n"
     ]
    }
   ],
   "source": [
    "# Example text document\n",
    "example_document = \"\"\"\n",
    "Natural language processing and text mining is a course in Master's degree program in the Univeristy of Oulu.\n",
    "Video games are good for passing time efficiently.\n",
    "University of Oulu has multiple restaurants which are cheap and offer healthy food.\n",
    "Multiple different courses require experience on programming languages.\n",
    "Oulu is known for many technological companies such as Oura and Fingersoft.\n",
    "\"\"\"\n",
    "parser = PlaintextParser.from_string(example_document, Tokenizer(LANGUAGE))\n",
    "\n",
    "print(\"--- Top 3 Most Important Sentences by Each Algorithm ---\")\n",
    "# TextRank summarization\n",
    "textrank_summarizer = TextRankSummarizer(Stemmer(LANGUAGE))\n",
    "textrank_summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "print(\"\\nTextRank Summary:\")\n",
    "for sentence in textrank_summarizer(parser.document, SENTENCES_COUNT):\n",
    "    print(sentence)\n",
    "\n",
    "# LSA summarization\n",
    "lsa_summarizer = LsaSummarizer(Stemmer(LANGUAGE))\n",
    "lsa_summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "print(\"\\nLSA Summary:\")\n",
    "for sentence in lsa_summarizer(parser.document, SENTENCES_COUNT):\n",
    "    print(sentence)\n",
    "\n",
    "# LexRank summarization\n",
    "lexrank_summarizer = LexRankSummarizer(Stemmer(LANGUAGE))\n",
    "lexrank_summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "print(\"\\nLexRank Summary:\")\n",
    "for sentence in lexrank_summarizer(parser.document, SENTENCES_COUNT):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fdece2",
   "metadata": {},
   "source": [
    "### Task 2: Text summarizer GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca9b500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Summarizer GUI is running on a separate window!\n"
     ]
    }
   ],
   "source": [
    "# The summarizer GUI will open in a new window and this cell will run as long as the window is open.\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, scrolledtext, messagebox\n",
    "\n",
    "# Helper function for text summarization\n",
    "def summarize_text(source, is_url=False):\n",
    "    try:\n",
    "        if is_url:\n",
    "            parser = HtmlParser.from_url(source, Tokenizer(LANGUAGE))\n",
    "        else:\n",
    "            parser = PlaintextParser.from_file(source, Tokenizer(LANGUAGE))\n",
    "        \n",
    "        # Initialize summarizers, use stemming and stop word removal\n",
    "        textrank = TextRankSummarizer(Stemmer(LANGUAGE))\n",
    "        textrank.stop_words = get_stop_words(LANGUAGE)\n",
    "        lsa = LsaSummarizer(Stemmer(LANGUAGE))\n",
    "        lsa.stop_words = get_stop_words(LANGUAGE)\n",
    "        lexrank = LexRankSummarizer(Stemmer(LANGUAGE))\n",
    "        lexrank.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "        # Create dictionary for summarizer outputs\n",
    "        summaries = {\n",
    "            \"TextRank\": \"\\n\\n\".join(str(s) for s in textrank(parser.document, SENTENCES_COUNT)),\n",
    "            \"LSA\": \"\\n\\n\".join(str(s) for s in lsa(parser.document, SENTENCES_COUNT)),\n",
    "            \"LexRank\": \"\\n\\n\".join(str(s) for s in lexrank(parser.document, SENTENCES_COUNT))\n",
    "        }\n",
    "        \n",
    "        return summaries\n",
    "\n",
    "    except Exception as e:\n",
    "        messagebox.showwarning(\"Input Error\", \"Please enter a URL or choose a file\")\n",
    "        print(\"Error: \", e)\n",
    "        return None\n",
    "\n",
    "# Helper function for browsing a file\n",
    "def browse_file():\n",
    "    filename = filedialog.askopenfilename(filetypes=[(\"Text files\", \"*.txt\")])\n",
    "    entry_source.delete(0, tk.END)\n",
    "    entry_source.insert(0, filename)\n",
    "\n",
    "# Helper function for running summary\n",
    "def run_summary():\n",
    "    source = entry_source.get().strip()\n",
    "    if not source:\n",
    "        messagebox.showwarning(\"Input Error\", \"Please enter a URL or choose a file\")\n",
    "        return None\n",
    "    \n",
    "    is_url = source.startswith(\"http\")\n",
    "    summaries = summarize_text(source, is_url)\n",
    "\n",
    "    # Delete old summaries and add new ones (if summarizers are run multiple times)\n",
    "    if summaries:\n",
    "        text_textrank.delete(1.0, tk.END)\n",
    "        text_lsa.delete(1.0, tk.END)\n",
    "        text_lexrank.delete(1.0, tk.END)\n",
    "        text_textrank.insert(tk.END, summaries[\"TextRank\"])\n",
    "        text_lsa.insert(tk.END, summaries[\"LSA\"])\n",
    "        text_lexrank.insert(tk.END, summaries[\"LexRank\"])\n",
    "\n",
    "# GUI setup using tkinter library\n",
    "root = tk.Tk()\n",
    "root.title(\"Text Summarizer\")\n",
    "root.geometry(\"1200x700\")\n",
    "\n",
    "frame_top = tk.Frame(root)\n",
    "frame_top.pack(pady=10)\n",
    "\n",
    "tk.Label(frame_top, text=\"Enter URL or choose a file to get 3 most important sentences:\").pack(anchor=\"w\", padx=5)\n",
    "entry_source = tk.Entry(frame_top, width=70)\n",
    "entry_source.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "btn_browse = tk.Button(frame_top, text=\"Browse File\", command=browse_file)\n",
    "btn_browse.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "btn_summarize = tk.Button(frame_top, text=\"Summarize\", command=run_summary)\n",
    "btn_summarize.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "# --Text areas for summaries--\n",
    "frame_texts = tk.Frame(root)\n",
    "frame_texts.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "# TextRank section\n",
    "frame_textrank = tk.Frame(frame_texts)\n",
    "frame_textrank.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)\n",
    "\n",
    "tk.Label(frame_textrank, text=\"TextRank Summary:\").pack(anchor=\"n\", pady=5)\n",
    "text_textrank = scrolledtext.ScrolledText(frame_textrank, wrap=tk.WORD, width=40)\n",
    "text_textrank.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "# LSA section\n",
    "frame_lsa = tk.Frame(frame_texts)\n",
    "frame_lsa.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)\n",
    "\n",
    "tk.Label(frame_lsa, text=\"LSA Summary:\").pack(pady=5)\n",
    "text_lsa = scrolledtext.ScrolledText(frame_lsa, wrap=tk.WORD, width=40)\n",
    "text_lsa.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "# LexRank section\n",
    "frame_lexrank = tk.Frame(frame_texts)\n",
    "frame_lexrank.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)\n",
    "\n",
    "tk.Label(frame_lexrank, text=\"LexRank Summary:\").pack(pady=5)\n",
    "text_lexrank = scrolledtext.ScrolledText(frame_lexrank, wrap=tk.WORD, width=40)\n",
    "text_lexrank.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "print(\"Text Summarizer GUI is running on a separate window!\")\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e658c8b",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5007ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abdd889",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f11c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b091141",
   "metadata": {},
   "source": [
    "### Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bbd702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cdee8c",
   "metadata": {},
   "source": [
    "### Task 6: Algorithm performance on Opinosis dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5f570ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Performance of TextRank, Latent Semantic and LexRank algorithms on Opinosis dataset in terms of Rouge-1 and Rouge-2 ---\n",
      "\n",
      "TextRank - ROUGE-1: 0.153 | ROUGE-2: 0.036\n",
      "\n",
      "LSA - ROUGE-1: 0.129 | ROUGE-2: 0.023\n",
      "\n",
      "LexRank - ROUGE-1: 0.238 | ROUGE-2: 0.066\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rouge_score import rouge_scorer\n",
    "from glob import glob\n",
    "\n",
    "# Helper function for text summarization\n",
    "def summarize_text(text, summarizer):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "    return \" \".join(str(s) for s in summarizer(parser.document, SENTENCES_COUNT))\n",
    "\n",
    "# Helper function for ROUGE evaluation\n",
    "def evaluate_rouge(system_summary, reference_summary):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\"], use_stemmer=True)\n",
    "    scores = scorer.score(reference_summary, system_summary)\n",
    "    return {k: v.fmeasure for k, v in scores.items()}\n",
    "\n",
    "# Setup summarizers (use stemming and stopword removal)\n",
    "summarizers = {\n",
    "    \"TextRank\": TextRankSummarizer(Stemmer(LANGUAGE)),\n",
    "    \"LSA\": LsaSummarizer(Stemmer(LANGUAGE)),\n",
    "    \"LexRank\": LexRankSummarizer(Stemmer(LANGUAGE))\n",
    "}\n",
    "for s in summarizers.values():\n",
    "    s.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "# Loop through the opinosis dataset\n",
    "topics_path = \"Data/OpinosisDataset/topics\"\n",
    "summaries_path = \"Data/OpinosisDataset/summaries-gold\"\n",
    "\n",
    "results = {name: [] for name in summarizers}\n",
    "\n",
    "for topic_file in os.listdir(topics_path):\n",
    "    if not topic_file.endswith(\".txt.data\"):\n",
    "        continue\n",
    "    \n",
    "    # Get the topic name from the file and construct paths for topics and gold summaries\n",
    "    topic_name = topic_file.replace(\"txt.data\", \"\")\n",
    "    topic_path = os.path.join(topics_path, topic_file)\n",
    "    summaries_dir = os.path.join(summaries_path, topic_name)\n",
    "\n",
    "    if not os.path.exists(summaries_dir):\n",
    "        continue\n",
    "    \n",
    "    # Read all reviews for the topic (there are mixed encodings in the dataset)\n",
    "    try:\n",
    "        with open(topic_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(topic_path, \"r\", encoding=\"latin-1\") as file:\n",
    "            text = file.read()\n",
    "    \n",
    "    # Read all gold summaries for the topic with glob\n",
    "    summaries_paths = sorted(glob(os.path.join(summaries_dir, \"*.gold\")))\n",
    "    gold_summaries = []\n",
    "    for s in summaries_paths:\n",
    "        with open(s, \"r\", encoding=\"utf-8\") as summary_file:\n",
    "            gold_summaries.append(summary_file.read())\n",
    "\n",
    "    # Evaluate each summarizer\n",
    "    for name, summarizer in summarizers.items():\n",
    "        system_summary = summarize_text(text, summarizer)\n",
    "        all_rouge = []\n",
    "        for gold in gold_summaries:\n",
    "            all_rouge.append(evaluate_rouge(system_summary, gold))\n",
    "        \n",
    "        # Compute average across all gold summaries\n",
    "        avg_rouge1 = sum(r[\"rouge1\"] for r in all_rouge) / len(all_rouge)\n",
    "        avg_rouge2 = sum(r[\"rouge2\"] for r in all_rouge) / len(all_rouge)\n",
    "        results[name].append({\"rouge1\": avg_rouge1, \"rouge2\": avg_rouge2})\n",
    "\n",
    "# Compute overall averages\n",
    "print (\"--- Performance of TextRank, Latent Semantic and LexRank algorithms on Opinosis dataset in terms of Rouge-1 and Rouge-2 ---\")\n",
    "for name, scores in results.items():\n",
    "    avg_r1 = sum(s[\"rouge1\"] for s in scores) / len(scores)\n",
    "    avg_r2 = sum(s[\"rouge2\"] for s in scores) / len(scores)\n",
    "    print(f\"\\n{name} - ROUGE-1: {avg_r1:.3f} | ROUGE-2: {avg_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35baa863",
   "metadata": {},
   "source": [
    "### Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a204879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58cbcf6",
   "metadata": {},
   "source": [
    "### Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90855de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eff6ea7",
   "metadata": {},
   "source": [
    "### Task 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf2efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
