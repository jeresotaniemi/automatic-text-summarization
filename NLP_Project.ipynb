{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f83328",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sumy rouge-score nltk lxml_html_clean spacy matplotlib  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ae305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and global variables\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "import spacy\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "# python -m spacy download en_core_web_sm <-- Run this command in terminal if the model is not already downloaded\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize language for stemmer and the number of most important sentences returned\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 3\n",
    "\n",
    "# **IMPORTANT Note**: Since we are using Sumy package instead of the original PyTLDR, there is no direct counterpart for Relevance sentence scoring. We use the closest equivalent LexRank which also uses cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e7a428",
   "metadata": {},
   "source": [
    "### Task 1: Three summarization algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3348da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text document\n",
    "example_document = \"\"\"\n",
    "Natural language processing and text mining is a course in Master's degree program in the Univeristy of Oulu.\n",
    "Video games are good for passing time efficiently.\n",
    "University of Oulu has multiple restaurants which are cheap and offer healthy food.\n",
    "Multiple different courses require experience on programming languages.\n",
    "Oulu is known for many technological companies such as Oura and Fingersoft.\n",
    "\"\"\"\n",
    "parser = PlaintextParser.from_string(example_document, Tokenizer(LANGUAGE))\n",
    "\n",
    "print(\"--- Top 3 Most Important Sentences by Each Algorithm ---\")\n",
    "# TextRank summarization\n",
    "textrank_summarizer = TextRankSummarizer(Stemmer(LANGUAGE))\n",
    "textrank_summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "print(\"\\nTextRank Summary:\")\n",
    "for sentence in textrank_summarizer(parser.document, SENTENCES_COUNT):\n",
    "    print(sentence)\n",
    "\n",
    "# LSA summarization\n",
    "lsa_summarizer = LsaSummarizer(Stemmer(LANGUAGE))\n",
    "lsa_summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "print(\"\\nLSA Summary:\")\n",
    "for sentence in lsa_summarizer(parser.document, SENTENCES_COUNT):\n",
    "    print(sentence)\n",
    "\n",
    "# LexRank summarization\n",
    "lexrank_summarizer = LexRankSummarizer(Stemmer(LANGUAGE))\n",
    "lexrank_summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "print(\"\\nLexRank Summary:\")\n",
    "for sentence in lexrank_summarizer(parser.document, SENTENCES_COUNT):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fdece2",
   "metadata": {},
   "source": [
    "### Task 2: Text summarizer GUI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca9b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The summarizer GUI will open in a new window and this cell will run as long as the window is open.\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, scrolledtext, messagebox\n",
    "\n",
    "\n",
    "# Helper function for text summarization\n",
    "def summarize_text(source, is_url=False):\n",
    "    try:\n",
    "        if is_url:\n",
    "            parser = HtmlParser.from_url(source, Tokenizer(LANGUAGE))\n",
    "        else:\n",
    "            parser = PlaintextParser.from_file(source, Tokenizer(LANGUAGE))\n",
    "\n",
    "        # Initialize summarizers, use stemming and stop word removal\n",
    "        textrank = TextRankSummarizer(Stemmer(LANGUAGE))\n",
    "        textrank.stop_words = get_stop_words(LANGUAGE)\n",
    "        lsa = LsaSummarizer(Stemmer(LANGUAGE))\n",
    "        lsa.stop_words = get_stop_words(LANGUAGE)\n",
    "        lexrank = LexRankSummarizer(Stemmer(LANGUAGE))\n",
    "        lexrank.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "        # Create dictionary for summarizer outputs\n",
    "        summaries = {\n",
    "            \"TextRank\": \"\\n\\n\".join(\n",
    "                str(s) for s in textrank(parser.document, SENTENCES_COUNT)\n",
    "            ),\n",
    "            \"LSA\": \"\\n\\n\".join(str(s) for s in lsa(parser.document, SENTENCES_COUNT)),\n",
    "            \"LexRank\": \"\\n\\n\".join(\n",
    "                str(s) for s in lexrank(parser.document, SENTENCES_COUNT)\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        return summaries\n",
    "\n",
    "    except Exception as e:\n",
    "        messagebox.showwarning(\"Input Error\", \"Please enter a URL or choose a file\")\n",
    "        print(\"Error: \", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "# Helper function for browsing a file\n",
    "def browse_file():\n",
    "    filename = filedialog.askopenfilename(filetypes=[(\"Text files\", \"*.txt\")])\n",
    "    entry_source.delete(0, tk.END)\n",
    "    entry_source.insert(0, filename)\n",
    "\n",
    "\n",
    "# Helper function for running summary\n",
    "def run_summary():\n",
    "    source = entry_source.get().strip()\n",
    "    if not source:\n",
    "        messagebox.showwarning(\"Input Error\", \"Please enter a URL or choose a file\")\n",
    "        return None\n",
    "\n",
    "    is_url = source.startswith(\"http\")\n",
    "    summaries = summarize_text(source, is_url)\n",
    "\n",
    "    # Delete old summaries and add new ones (if summarizers are run multiple times)\n",
    "    if summaries:\n",
    "        text_textrank.delete(1.0, tk.END)\n",
    "        text_lsa.delete(1.0, tk.END)\n",
    "        text_lexrank.delete(1.0, tk.END)\n",
    "        text_textrank.insert(tk.END, summaries[\"TextRank\"])\n",
    "        text_lsa.insert(tk.END, summaries[\"LSA\"])\n",
    "        text_lexrank.insert(tk.END, summaries[\"LexRank\"])\n",
    "\n",
    "\n",
    "# GUI setup using tkinter library\n",
    "root = tk.Tk()\n",
    "root.title(\"Text Summarizer\")\n",
    "root.geometry(\"1200x700\")\n",
    "\n",
    "frame_top = tk.Frame(root)\n",
    "frame_top.pack(pady=10)\n",
    "\n",
    "tk.Label(\n",
    "    frame_top, text=\"Enter URL or choose a file to get 3 most important sentences:\"\n",
    ").pack(anchor=\"w\", padx=5)\n",
    "entry_source = tk.Entry(frame_top, width=70)\n",
    "entry_source.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "btn_browse = tk.Button(frame_top, text=\"Browse File\", command=browse_file)\n",
    "btn_browse.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "btn_summarize = tk.Button(frame_top, text=\"Summarize\", command=run_summary)\n",
    "btn_summarize.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "# --Text areas for summaries--\n",
    "frame_texts = tk.Frame(root)\n",
    "frame_texts.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "# TextRank section\n",
    "frame_textrank = tk.Frame(frame_texts)\n",
    "frame_textrank.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)\n",
    "\n",
    "tk.Label(frame_textrank, text=\"TextRank Summary:\").pack(anchor=\"n\", pady=5)\n",
    "text_textrank = scrolledtext.ScrolledText(frame_textrank, wrap=tk.WORD, width=40)\n",
    "text_textrank.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "# LSA section\n",
    "frame_lsa = tk.Frame(frame_texts)\n",
    "frame_lsa.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)\n",
    "\n",
    "tk.Label(frame_lsa, text=\"LSA Summary:\").pack(pady=5)\n",
    "text_lsa = scrolledtext.ScrolledText(frame_lsa, wrap=tk.WORD, width=40)\n",
    "text_lsa.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "# LexRank section\n",
    "frame_lexrank = tk.Frame(frame_texts)\n",
    "frame_lexrank.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)\n",
    "\n",
    "tk.Label(frame_lexrank, text=\"LexRank Summary:\").pack(pady=5)\n",
    "text_lexrank = scrolledtext.ScrolledText(frame_lexrank, wrap=tk.WORD, width=40)\n",
    "text_lexrank.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "print(\"Text Summarizer GUI is running on a separate window!\")\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e658c8b",
   "metadata": {},
   "source": [
    "### Task 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5007ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_doc_and_abstraction(data):\n",
    "    extracted_data = []\n",
    "    data_idx = -1\n",
    "    for idx, line in enumerate(data):\n",
    "        if line == \"Document\\n\":\n",
    "            data_idx += 1\n",
    "            extracted_data.append({\"document\": data[idx + 1].rstrip(\"\\n\")})\n",
    "            continue\n",
    "        if line.startswith(\"Facet-\"):\n",
    "            abstraction = line.rstrip(\"\\n\").split(\" \", 1)[1]\n",
    "            if \"abstractions\" not in extracted_data[data_idx]:\n",
    "                extracted_data[data_idx][\"abstractions\"] = []\n",
    "            extracted_data[data_idx][\"abstractions\"].append(abstraction)\n",
    "    print(f\"Total extracted samples: {len(extracted_data)}\")\n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def rouge_evaluation(reference, hypothesis, rouge_type):\n",
    "    scorer = rouge_scorer.RougeScorer([rouge_type], use_stemmer=True)\n",
    "    score = scorer.score(reference, hypothesis)[rouge_type]\n",
    "    return score\n",
    "\n",
    "\n",
    "with open(\"Data/Cnn&Dailymail/low_abstraction.txt\", \"r\") as f:\n",
    "    data = extract_doc_and_abstraction(f.readlines())\n",
    "\n",
    "\n",
    "def get_avg_rouge_scores(abstractions, summary, rouge_type):\n",
    "    scores = [rouge_evaluation(abst, str(summary), rouge_type) for abst in abstractions]\n",
    "    precision = np.mean([score.precision for score in scores])\n",
    "    recall = np.mean([score.recall for score in scores])\n",
    "    fmeasure = np.mean([score.fmeasure for score in scores])\n",
    "    return precision, recall, fmeasure\n",
    "\n",
    "\n",
    "textrank_summarizer = TextRankSummarizer(Stemmer(LANGUAGE))\n",
    "textrank_summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "lsa_summarizer = LsaSummarizer(Stemmer(LANGUAGE))\n",
    "lsa_summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "lexrank_summarizer = LexRankSummarizer(Stemmer(LANGUAGE))\n",
    "lexrank_summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "summarizers = [\"TextRank\", \"LSA\", \"LexRank\"]\n",
    "rouge_types = [\"rouge2\", \"rouge3\"]\n",
    "metrics = [\"precision\", \"recall\", \"fmeasure\"]\n",
    "\n",
    "TOP_SUMMARIES_COUNT = 5\n",
    "\n",
    "doc_scores = []\n",
    "summary_keys = [f\"summary{i + 1}\" for i in range(TOP_SUMMARIES_COUNT)]\n",
    "score_template = {\n",
    "    f\"{summary_key}\": {\n",
    "        f\"{summarizer}\": {\n",
    "            f\"{rouge_type}\": {metric: 0 for metric in metrics}\n",
    "            for rouge_type in rouge_types\n",
    "        }\n",
    "        for summarizer in summarizers\n",
    "    }\n",
    "    for summary_key in summary_keys\n",
    "}\n",
    "\n",
    "for doc in data:\n",
    "    scores = deepcopy(score_template)\n",
    "    document = doc[\"document\"]\n",
    "    abstractions = doc[\"abstractions\"]\n",
    "    parser = PlaintextParser.from_string(document, Tokenizer(LANGUAGE))\n",
    "\n",
    "    summaries = {f\"{summarizer}\": [] for summarizer in summarizers}\n",
    "    summaries[\"TextRank\"] = textrank_summarizer(parser.document, TOP_SUMMARIES_COUNT)\n",
    "    summaries[\"LSA\"] = lsa_summarizer(parser.document, TOP_SUMMARIES_COUNT)\n",
    "    summaries[\"LexRank\"] = lexrank_summarizer(parser.document, TOP_SUMMARIES_COUNT)\n",
    "\n",
    "    for summary_idx in range(TOP_SUMMARIES_COUNT):\n",
    "        for summarizer in summarizers:\n",
    "            for rouge_type in rouge_types:\n",
    "                summary_key = f\"summary{summary_idx + 1}\"\n",
    "                precision, recall, fmeasure = get_avg_rouge_scores(\n",
    "                    abstractions, summaries[summarizer][summary_idx], rouge_type\n",
    "                )\n",
    "                scores[summary_key][summarizer][rouge_type][\"precision\"] = precision\n",
    "                scores[summary_key][summarizer][rouge_type][\"recall\"] = recall\n",
    "                scores[summary_key][summarizer][rouge_type][\"fmeasure\"] = fmeasure\n",
    "\n",
    "    doc_scores.append(scores)\n",
    "\n",
    "\n",
    "avg_scores = deepcopy(score_template)\n",
    "\n",
    "for doc_score in doc_scores:\n",
    "    for summary_key in summary_keys:\n",
    "        for summarizer in summarizers:\n",
    "            for rouge_type in rouge_types:\n",
    "                avg_scores[summary_key][summarizer][rouge_type][\"precision\"] += (\n",
    "                    doc_score[summary_key][summarizer][rouge_type][\"precision\"]\n",
    "                    / len(doc_scores)\n",
    "                )\n",
    "                avg_scores[summary_key][summarizer][rouge_type][\"recall\"] += doc_score[\n",
    "                    summary_key\n",
    "                ][summarizer][rouge_type][\"recall\"] / len(doc_scores)\n",
    "                avg_scores[summary_key][summarizer][rouge_type][\"fmeasure\"] += (\n",
    "                    doc_score[summary_key][summarizer][rouge_type][\"fmeasure\"]\n",
    "                    / len(doc_scores)\n",
    "                )\n",
    "\n",
    "\n",
    "def plot_rouge_scores(ax, rouge_type, metric):\n",
    "    summary_indices = range(1, TOP_SUMMARIES_COUNT + 1)\n",
    "    ax.set_title(f\"Average {rouge_type.upper()} scores ({metric})\")\n",
    "    ax.set_xlabel(\"Summary Index\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_xticks(summary_indices)\n",
    "    for summarizer in summarizers:\n",
    "        scores = [\n",
    "            avg_scores[f\"summary{idx}\"][summarizer][rouge_type][metric]\n",
    "            for idx in summary_indices\n",
    "        ]\n",
    "        ax.plot(summary_indices, scores, marker=\"o\", label=summarizer)\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "\n",
    "# Plot the average ROUGE scores\n",
    "_, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(14, 21))\n",
    "axes = [ax1, ax2, ax3, ax4, ax5, ax6]\n",
    "for i, (rouge_type, metric) in enumerate(\n",
    "    [\n",
    "        (\"rouge2\", \"fmeasure\"),\n",
    "        (\"rouge3\", \"fmeasure\"),\n",
    "        (\"rouge2\", \"precision\"),\n",
    "        (\"rouge3\", \"precision\"),\n",
    "        (\"rouge2\", \"recall\"),\n",
    "        (\"rouge3\", \"recall\"),\n",
    "    ]\n",
    "):\n",
    "    plot_rouge_scores(axes[i], rouge_type, metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abdd889",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "We want to extend the above summarization by incorporating coherence of text with respect to  \n",
    "named-entity. For this purpose, first use SpaCy named-entity tagger and identify person or  \n",
    "organization named-entity. Suggest a simple heuristic that enables whenever a sentence outputted by  \n",
    "a given algorithm contains a person or an organization named-entity, then other sentences in the  \n",
    "original document that contain the same named-entity, if not outputted by the underlined algorithm,  \n",
    "will also be included in the summarizer up to a certain threshold (that you can discuss and tune up).  \n",
    "Run the newly designed algorithm on the same CNN/Dailymail dataset, and report the ROUGE-2 and  \n",
    "ROUGE-3 performances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f11c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_SENTENCES = 3\n",
    "\n",
    "\n",
    "def extract_doc_and_abstraction(data):\n",
    "    extracted_data = []\n",
    "    data_idx = -1\n",
    "    for idx, line in enumerate(data):\n",
    "        if line == \"Document\\n\":\n",
    "            data_idx += 1\n",
    "            extracted_data.append({\"document\": data[idx + 1].rstrip(\"\\n\")})\n",
    "            continue\n",
    "        if line.startswith(\"Facet-\"):\n",
    "            abstraction = line.rstrip(\"\\n\").split(\" \", 1)[1]\n",
    "            if \"abstractions\" not in extracted_data[data_idx]:\n",
    "                extracted_data[data_idx][\"abstractions\"] = []\n",
    "            extracted_data[data_idx][\"abstractions\"].append(abstraction)\n",
    "    print(f\"Total extracted samples: {len(extracted_data)}\")\n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "with open(\"Data/Cnn&Dailymail/low_abstraction.txt\", \"r\") as f:\n",
    "    data = extract_doc_and_abstraction(f.readlines())\n",
    "\n",
    "\n",
    "def rouge_evaluation(reference, hypothesis, rouge_type):\n",
    "    scorer = rouge_scorer.RougeScorer([rouge_type], use_stemmer=True)\n",
    "    score = scorer.score(reference, hypothesis)[rouge_type]\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_avg_rouge_scores(abstractions, summary, rouge_type):\n",
    "    scores = [rouge_evaluation(abst, str(summary), rouge_type) for abst in abstractions]\n",
    "    precision = np.mean([score.precision for score in scores])\n",
    "    recall = np.mean([score.recall for score in scores])\n",
    "    fmeasure = np.mean([score.fmeasure for score in scores])\n",
    "    return precision, recall, fmeasure\n",
    "\n",
    "\n",
    "def add_sents_containing_named_entities(summary, document, max_new_sentences):\n",
    "    summary_doc = nlp(str(summary))\n",
    "    summary_entities = {\n",
    "        ent.text for ent in summary_doc.ents if ent.label_ in (\"PERSON\", \"ORG\")\n",
    "    }\n",
    "\n",
    "    if not summary_entities:\n",
    "        return str(summary)\n",
    "\n",
    "    document_doc = nlp(document)\n",
    "    document_sents = [sent.text.strip() for sent in document_doc.sents]\n",
    "\n",
    "    summary_text = str(summary)\n",
    "    candidates = []\n",
    "\n",
    "    for sent in document_sents:\n",
    "        # Skip if it's the summary sentence itself\n",
    "        if sent == summary_text:\n",
    "            continue\n",
    "\n",
    "        sent_doc = nlp(sent)\n",
    "        sent_entities = {\n",
    "            ent.text for ent in sent_doc.ents if ent.label_ in (\"PERSON\", \"ORG\")\n",
    "        }\n",
    "\n",
    "        overlap = summary_entities & sent_entities\n",
    "\n",
    "        if overlap:\n",
    "            candidates.append((sent, len(overlap)))\n",
    "\n",
    "    # Sort by number of matching entities (descending)\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Take top max_new_sentences\n",
    "    selected_sentences = [summary_text]\n",
    "    selected_sentences.extend(sent for sent, _ in candidates[:max_new_sentences])\n",
    "\n",
    "    enhanced_summary = \" \".join(selected_sentences)\n",
    "\n",
    "    return enhanced_summary\n",
    "\n",
    "\n",
    "textrank_summarizer = TextRankSummarizer(Stemmer(LANGUAGE))\n",
    "textrank_summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "lsa_summarizer = LsaSummarizer(Stemmer(LANGUAGE))\n",
    "lsa_summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "lexrank_summarizer = LexRankSummarizer(Stemmer(LANGUAGE))\n",
    "lexrank_summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "summarizers = [\"TextRank\", \"LSA\", \"LexRank\"]\n",
    "rouge_types = [\"rouge2\", \"rouge3\"]\n",
    "metrics = [\"precision\", \"recall\", \"fmeasure\"]\n",
    "\n",
    "TOP_SUMMARIES_COUNT = 1\n",
    "\n",
    "doc_scores = []\n",
    "score_template = {\n",
    "    f\"{summarizer}\": {\n",
    "        f\"{rouge_type}\": {metric: 0 for metric in metrics} for rouge_type in rouge_types\n",
    "    }\n",
    "    for summarizer in summarizers\n",
    "}\n",
    "\n",
    "for doc in data:\n",
    "    scores = deepcopy(score_template)\n",
    "    document = doc[\"document\"]\n",
    "    abstractions = doc[\"abstractions\"]\n",
    "    parser = PlaintextParser.from_string(document, Tokenizer(LANGUAGE))\n",
    "\n",
    "    summaries = {f\"{summarizer}\": [] for summarizer in summarizers}\n",
    "    summaries[\"TextRank\"] = textrank_summarizer(parser.document, TOP_SUMMARIES_COUNT)[0]\n",
    "    summaries[\"LSA\"] = lsa_summarizer(parser.document, TOP_SUMMARIES_COUNT)[0]\n",
    "    summaries[\"LexRank\"] = lexrank_summarizer(parser.document, TOP_SUMMARIES_COUNT)[0]\n",
    "\n",
    "    summaries[\"TextRank\"] = add_sents_containing_named_entities(\n",
    "        summaries[\"TextRank\"], document, MAX_NEW_SENTENCES\n",
    "    )\n",
    "    summaries[\"LSA\"] = add_sents_containing_named_entities(\n",
    "        summaries[\"LSA\"], document, MAX_NEW_SENTENCES\n",
    "    )\n",
    "    summaries[\"LexRank\"] = add_sents_containing_named_entities(\n",
    "        summaries[\"LexRank\"], document, MAX_NEW_SENTENCES\n",
    "    )\n",
    "\n",
    "    for summarizer in summarizers:\n",
    "        for rouge_type in rouge_types:\n",
    "            precision, recall, fmeasure = get_avg_rouge_scores(\n",
    "                abstractions, summaries[summarizer], rouge_type\n",
    "            )\n",
    "            scores[summarizer][rouge_type][\"precision\"] = precision\n",
    "            scores[summarizer][rouge_type][\"recall\"] = recall\n",
    "            scores[summarizer][rouge_type][\"fmeasure\"] = fmeasure\n",
    "\n",
    "    doc_scores.append(scores)\n",
    "\n",
    "\n",
    "avg_scores = deepcopy(score_template)\n",
    "\n",
    "for doc_score in doc_scores:\n",
    "    for summarizer in summarizers:\n",
    "        for rouge_type in rouge_types:\n",
    "            avg_scores[summarizer][rouge_type][\"precision\"] += doc_score[summarizer][\n",
    "                rouge_type\n",
    "            ][\"precision\"] / len(doc_scores)\n",
    "            avg_scores[summarizer][rouge_type][\"recall\"] += doc_score[summarizer][\n",
    "                rouge_type\n",
    "            ][\"recall\"] / len(doc_scores)\n",
    "            avg_scores[summarizer][rouge_type][\"fmeasure\"] += doc_score[summarizer][\n",
    "                rouge_type\n",
    "            ][\"fmeasure\"] / len(doc_scores)\n",
    "\n",
    "\n",
    "for summarizer in summarizers:\n",
    "    for rouge_type in rouge_types:\n",
    "        print(\n",
    "            f\"{summarizer} - {rouge_type} - Precision: {avg_scores[summarizer][rouge_type]['precision']:.4f}, \"\n",
    "            f\"Recall: {avg_scores[summarizer][rouge_type]['recall']:.4f}, \"\n",
    "            f\"F-measure: {avg_scores[summarizer][rouge_type]['fmeasure']:.4f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_rouge_scores(ax, metric):\n",
    "    x_positions = range(1, len(rouge_types) + 1)\n",
    "    x_ticklabels = [\"Rouge-2\", \"Rouge-3\"]\n",
    "    ax.set_title(f\"Average ROUGE scores ({metric})\")\n",
    "    ax.set_xlabel(\"Rouge Type\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels(x_ticklabels)\n",
    "    for summarizer in summarizers:\n",
    "        scores = [\n",
    "            avg_scores[summarizer][rouge_type][metric] for rouge_type in rouge_types\n",
    "        ]\n",
    "        ax.plot(x_positions, scores, marker=\"o\", label=summarizer)\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "\n",
    "# Plot the average ROUGE scores\n",
    "_, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, metric in enumerate(metrics):\n",
    "    plot_rouge_scores(axes[i], metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b091141",
   "metadata": {},
   "source": [
    "### Task 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bbd702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cdee8c",
   "metadata": {},
   "source": [
    "### Task 6: Algorithm performance on Opinosis dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f570ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from rouge_score import rouge_scorer\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "# Helper function for text summarization\n",
    "def summarize_text(text, summarizer):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "    return \" \".join(str(s) for s in summarizer(parser.document, SENTENCES_COUNT))\n",
    "\n",
    "\n",
    "# Helper function for ROUGE evaluation\n",
    "def evaluate_rouge(system_summary, reference_summary):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\"], use_stemmer=True)\n",
    "    scores = scorer.score(reference_summary, system_summary)\n",
    "    return {k: v.fmeasure for k, v in scores.items()}\n",
    "\n",
    "\n",
    "# Setup summarizers (use stemming and stopword removal)\n",
    "summarizers = {\n",
    "    \"TextRank\": TextRankSummarizer(Stemmer(LANGUAGE)),\n",
    "    \"LSA\": LsaSummarizer(Stemmer(LANGUAGE)),\n",
    "    \"LexRank\": LexRankSummarizer(Stemmer(LANGUAGE)),\n",
    "}\n",
    "for s in summarizers.values():\n",
    "    s.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "# Loop through the opinosis dataset\n",
    "topics_path = \"Data/OpinosisDataset/topics\"\n",
    "summaries_path = \"Data/OpinosisDataset/summaries-gold\"\n",
    "\n",
    "result_list = {name: [] for name in summarizers}\n",
    "\n",
    "for topic_file in os.listdir(topics_path):\n",
    "    if not topic_file.endswith(\".txt.data\"):\n",
    "        continue\n",
    "\n",
    "    # Get the topic name from the file and construct paths for topics and gold summaries\n",
    "    topic_name = topic_file.replace(\"txt.data\", \"\")\n",
    "    topic_path = os.path.join(topics_path, topic_file)\n",
    "    summaries_dir = os.path.join(summaries_path, topic_name)\n",
    "\n",
    "    if not os.path.exists(summaries_dir):\n",
    "        continue\n",
    "\n",
    "    # Read all reviews for the topic (there are mixed encodings in the dataset)\n",
    "    try:\n",
    "        with open(topic_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(topic_path, \"r\", encoding=\"latin-1\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "    # Read all gold summaries for the topic with glob\n",
    "    summaries_paths = sorted(glob(os.path.join(summaries_dir, \"*.gold\")))\n",
    "    gold_summaries = []\n",
    "    for s in summaries_paths:\n",
    "        with open(s, \"r\", encoding=\"utf-8\") as summary_file:\n",
    "            gold_summaries.append(summary_file.read())\n",
    "\n",
    "    # Evaluate each summarizer\n",
    "    for name, summarizer in summarizers.items():\n",
    "        system_summary = summarize_text(text, summarizer)\n",
    "        all_rouge = []\n",
    "        for gold in gold_summaries:\n",
    "            all_rouge.append(evaluate_rouge(system_summary, gold))\n",
    "\n",
    "        # Compute average across all gold summaries\n",
    "        avg_rouge1 = sum(r[\"rouge1\"] for r in all_rouge) / len(all_rouge)\n",
    "        avg_rouge2 = sum(r[\"rouge2\"] for r in all_rouge) / len(all_rouge)\n",
    "        result_list[name].append({\"rouge1\": avg_rouge1, \"rouge2\": avg_rouge2})\n",
    "\n",
    "# Compute overall averages\n",
    "print(\n",
    "    \"--- Performance of TextRank, Latent Semantic and LexRank algorithms on Opinosis dataset in terms of Rouge-1 and Rouge-2 ---\"\n",
    ")\n",
    "for name, textrank_scores in result_list.items():\n",
    "    avg_r1 = sum(s[\"rouge1\"] for s in textrank_scores) / len(textrank_scores)\n",
    "    avg_r2 = sum(s[\"rouge2\"] for s in textrank_scores) / len(textrank_scores)\n",
    "    print(f\"\\n{name} - ROUGE-1: {avg_r1:.3f} | ROUGE-2: {avg_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35baa863",
   "metadata": {},
   "source": [
    "### Task 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a204879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.edmundson import EdmundsonSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "\n",
    "\n",
    "def edmundson_summarize():\n",
    "    # Initialize Edmundson summarizer\n",
    "    # Has default weights: cue = 1, key = 0, title = 1, location = 1\n",
    "    summarizer = EdmundsonSummarizer(Stemmer(LANGUAGE))\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "    # Set null words (irrelevant words)\n",
    "    summarizer.null_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "    # Set custom bonus and stigma words\n",
    "    # For bonus words, include positively relevant words in reviews\n",
    "    summarizer.bonus_words = {\n",
    "        \"best\",\n",
    "        \"excellent\",\n",
    "        \"great\",\n",
    "        \"good\",\n",
    "        \"nice\",\n",
    "        \"perfect\",\n",
    "        \"better\",\n",
    "        \"awesome\",\n",
    "        \"recommend\",\n",
    "        \"super\",\n",
    "        \"fantastic\",\n",
    "        \"exceptional\",\n",
    "        \"outstanding\",\n",
    "    }\n",
    "    # For stigma words, include negatively relevant words in reviews\n",
    "    summarizer.stigma_words = {\n",
    "        \"bad\",\n",
    "        \"worst\",\n",
    "        \"terrible\",\n",
    "        \"poor\",\n",
    "        \"horrible\",\n",
    "        \"disappointing\",\n",
    "        \"avoid\",\n",
    "        \"unfortunately\",\n",
    "        \"cannot\",\n",
    "        \"useless\",\n",
    "        \"problem\",\n",
    "        \"issue\",\n",
    "        \"awful\",\n",
    "    }\n",
    "\n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\"], use_stemmer=True)\n",
    "    results = []\n",
    "    # Process each topic file\n",
    "    for topic_file in os.listdir(topics_path):\n",
    "        if not topic_file.endswith(\".txt.data\"):\n",
    "            continue\n",
    "        topic_name = topic_file.replace(\".txt.data\", \"\")\n",
    "        topic_path = os.path.join(topics_path, topic_file)\n",
    "        summaries_dir = os.path.join(summaries_path, topic_name)\n",
    "        if not os.path.exists(summaries_dir):\n",
    "            continue\n",
    "        # Read all reviews\n",
    "        try:\n",
    "            with open(topic_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(topic_path, \"r\", encoding=\"latin-1\") as file:\n",
    "                text = file.read()\n",
    "        # Generate summary\n",
    "        parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "        summary = \" \".join(\n",
    "            str(sentence) for sentence in summarizer(parser.document, SENTENCES_COUNT)\n",
    "        )\n",
    "        # Read and evaluate against gold summaries\n",
    "        summaries_paths = sorted(glob(os.path.join(summaries_dir, \"*.gold\")))\n",
    "        scores_for_topic = []\n",
    "        for gold_path in summaries_paths:\n",
    "            with open(gold_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                gold_summary = f.read()\n",
    "                scores = scorer.score(gold_summary, summary)\n",
    "                scores_for_topic.append(\n",
    "                    {\n",
    "                        \"rouge1\": scores[\"rouge1\"].fmeasure,\n",
    "                        \"rouge2\": scores[\"rouge2\"].fmeasure,\n",
    "                    }\n",
    "                )\n",
    "        # Compute average scores\n",
    "        if scores_for_topic:\n",
    "            avg_rouge1 = sum(score[\"rouge1\"] for score in scores_for_topic) / len(\n",
    "                scores_for_topic\n",
    "            )\n",
    "            avg_rouge2 = sum(score[\"rouge2\"] for score in scores_for_topic) / len(\n",
    "                scores_for_topic\n",
    "            )\n",
    "            results.append({\"rouge1\": avg_rouge1, \"rouge2\": avg_rouge2})\n",
    "    # Compute and output results\n",
    "    if results:\n",
    "        final_rouge1 = sum(result[\"rouge1\"] for result in results) / len(results)\n",
    "        final_rouge2 = sum(result[\"rouge2\"] for result in results) / len(results)\n",
    "        print(\"\\nSumy Edmundson summarizer ROUGE-1, ROUGE-2 scores:\")\n",
    "        print(f\"ROUGE-1: {final_rouge1:.3f}\")\n",
    "        print(f\"ROUGE-2: {final_rouge2:.3f}\")\n",
    "    else:\n",
    "        print(\"No results generated\")\n",
    "\n",
    "\n",
    "print(\"Test the summarizer on Opinosis dataset:\")\n",
    "edmundson_summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58cbcf6",
   "metadata": {},
   "source": [
    "### Task 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90855de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eff6ea7",
   "metadata": {},
   "source": [
    "### Task 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf2efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
